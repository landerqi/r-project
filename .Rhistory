setwd("~/")
setwd("E:/www.github.com/r-project")
# 【数据获取】爬虫利器Rvest包
library(rvest)
web <- read_html("https://book.douban.com/top250?icn=index-book250-all", encoding="UTF-8")
web
position <- web %>% html_nodes("p.pl") %>% html_text()
# position <- iconv(position, from = 'UTF-8', to = 'GB2312')
Encoding(position)
position # 有时输出的中文是 unicode码 此时需要设置：Sys.setlocale("LC_CTYPE", "UTF-8")
# position <- iconv(position, from = 'UTF-8', to = 'GB2312')
Encoding(position)
position # 有时输出的中文是 unicode码 此时需要设置：Sys.setlocale("LC_CTYPE", "UTF-8")
# 获取字符串的长度
fruit <- 'apple orange grape banana'
nchar(fruit)
length(fruit)
# 字符串分割
strsplit(fruit, split = ' ')
fruitvec <- unlist(strsplit(fruit, split = ' '))
fruitvec <- unlist(strsplit(fruit, split = ' ')); fruitvec
# 字符串拼接
paste(fruitvec, collapse = ',')
# 字符串截取
substr(fruit, 1, 5)
# 字符串替代
gsub('apple', 'strawberry', fruit)
# 字符串匹配
grep('grape', fruitvec)
grepl('grape', fruitvec) # 返回位置
load("E:/R/.RData")
setwd("E:/www.github.com/r-project")
# 小练习
### 读取用户r语言已经安装的每个扩展包的DESCRIPTION文件。
path <- .libPaths()[1]
path
# 小练习
### 读取用户r语言已经安装的每个扩展包的DESCRIPTION文件。
path <- .libPaths()[1]; path
# 小练习
### 读取用户r语言已经安装的每个扩展包的DESCRIPTION文件。
path <- .libPaths()[1]; path
doc.names <- dir(path)
doc.names <- dir(path); doc.names
# 小练习
### 读取用户r语言已经安装的每个扩展包的DESCRIPTION文件。
path <- .libPaths()[1]; path
doc.path <- sapply(doc.names, function(names) paste(path, names, 'DESCRIPTION', sep = '/'))
doc.path
doc <- sapply(doc.path, function(doc) readLines(doc))
doc
# 小练习：R官网上的扩展包信息
library(XML)
web <- 'https://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- readHTMLTable(web, stringsAsFactors = FALSE)
packages
web <- 'http://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- readHTMLTable(web, stringsAsFactors = FALSE)
packages
web <- 'http://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- readHTMLTable(web, stringsAsFactors = FALSE)
packages
library (RCurl)
install.packages("RCurl")
library (RCurl)
library (XML)
curlVersion()$features
curlVersion()$protocol
## These should show ssl and https.
## I can see these on windows 8.1 at least.
## It may differ on other OSes.
temp <- getURL(web,
ssl.verifyPeer=FALSE)
DFX <- xmlTreeParse(temp,useInternal = TRUE)
DFX
DFX <- xmlTreeParse(temp, useInternal = TRUE)
# 小练习：R官网上的扩展包信息
library(rvest)
packages <- read_html(web)
packages
position <- packages %>% html_nodes("tbody tr td a") %>% html_text()
position
position <- packages %>% html_nodes("table tbody tr td a") %>% html_text()
position
position <- packages %>% html_nodes(xpath = '/html/body/table/tbody/tr[2]/td[1]/a') %>% html_text()
position
packages <- read_html(web)
# 小练习：R官网上的扩展包信息
library(rvest)
web <- 'https://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- read_html(web)
position <- packages %>% html_nodes(xpath = '/html/body/table/tbody/tr[2]/td[1]/a') %>% html_text()
position
position <- packages %>% html_nodes(xpath = '/html/body/table/tbody/tr[2]/td[1]/a')
position
packages <- read_html(web)
position <- packages %>% html_nodes('tbody') %>% html_nodes('tr') %>% html_nodes('td') %>% html_nodes('a') %>% html_text()
position
head(packages)
packages <- read_html(web); packages
a <- packages %>% html_node('h1') %>% html_text(); a
a <- packages %>% html_node('p') %>% html_text(); a
a <- packages %>% html_nodes('h1') %>% html_text(); a
position <- packages %>% html_node('tbody') %>% html_node('tr') %>% html_node('td') %>% html_node('a') %>% html_text()
position
web <- 'https://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- html_table(web); packages
packages <- read_html(web); packages
a <- packages %>% html_node('table') %>% html_table(header = FALSE); a
head(a)
a <- packages %>% html_node('table') %>% html_table(header = FALSE);
head(a)
position
###
births <- read_html("https://www.ssa.gov/oact/babynames/numberUSbirths.html")
html_table(html_nodes(births, "table")[[2]])
web <- 'https://cran.r-project.org/web/packages/available_packages_by_name.html'
packages <- read_html(web);
webhtml <- read_html(web);
webhtml <- read_html(web);
packages <- webhtml %>% html_node('table') %>% html_table(header = FALSE);
pnames <- packages[[1]][ ,1]
length(packages)
packages
webhtml
pnames <- html_table(html_nodes(webhtml, "table")[[1]][ ,1])
births <- read_html("https://www.ssa.gov/oact/babynames/numberUSbirths.html")
html_table(html_nodes(births, "table")[[2]])
tdist <- read_html("http://en.wikipedia.org/wiki/Student%27s_t-distribution")
tdist %>%
html_node("table.infobox") %>%
html_table(header = FALSE)
mylist <- as.list(iris[ ,1:4])
class(mylist)
mylist <- as.list(iris[ ,1:4]); mylist
vec <- round(runif(12) * 100)
vec
mat <- matrix(vec, 3, 4)
mat
apply(mat, 1, sum)
apply(mat, 2, function(x) max(x) - min(x))
# 数据的拆分合并
### 合并数据框
datax <- data.frame(id = c(1, 2, 3), gender = c(23, 34, 41))
datay <- data.frame(id = c(3, 1, 2), name = c('tom', 'john', 'ken'))
# 数据的拆分合并
### 合并数据框
datax <- data.frame(id = c(1, 2, 3), gender = c(23, 34, 41)); datax
datay <- data.frame(id = c(3, 1, 2), name = c('tom', 'john', 'ken')); datay
class(iris)
# 数据的拆分合并
### 合并数据框
datax <- data.frame(id = c(1, 2, 3), gender = c(23, 34, 41)); datax
datay <- data.frame(id = c(3, 1, 2), name = c('tom', 'john', 'ken')); datay
merge(datax, datay, by = 'id')
5 %% 3
6 %% 3
5 %m% 3
5 %>% 3
5 %in% 3
